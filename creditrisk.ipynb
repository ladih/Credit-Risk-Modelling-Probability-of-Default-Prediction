{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Credit Risk Modelling: Probability of Default Prediction\n",
    "\n",
    "This project uses logistic regression to predict probability of borrower default (PD) using Lending Club loan data. It includes data preparation and evaluation tools such as AUC, KS statistic, PSI, and calibration.\n",
    "\n",
    "**Data source:** Lending Club loan data, https://www.kaggle.com/datasets/adarshsng/lending-club-loan-data-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "1. Preparation\n",
    "2. Model training\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "df_loans = pd.read_csv('data/loan.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loans: 2260668\n",
      "Number of features: 145\n"
     ]
    }
   ],
   "source": [
    "# Number of loans and features\n",
    "print(\"Number of loans:\", df_loans.shape[0])\n",
    "print(\"Number of features:\", df_loans.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example loan:\n",
      "\n",
      "id                                            nan\n",
      "member_id                                     nan\n",
      "loan_amnt                                     2500\n",
      "funded_amnt                                   2500\n",
      "funded_amnt_inv                               2500.0\n",
      "term                                           36 months\n",
      "int_rate                                      13.56\n",
      "installment                                   84.92\n",
      "grade                                         C\n",
      "sub_grade                                     C1\n",
      "emp_title                                     Chef\n",
      "emp_length                                    10+ years\n",
      "home_ownership                                RENT\n",
      "annual_inc                                    55000.0\n",
      "verification_status                           Not Verified\n",
      "issue_d                                       Dec-2018\n",
      "loan_status                                   Current\n",
      "pymnt_plan                                    n\n",
      "url                                           nan\n",
      "desc                                          nan\n",
      "purpose                                       debt_consolidation\n",
      "title                                         Debt consolidation\n",
      "zip_code                                      109xx\n",
      "addr_state                                    NY\n",
      "dti                                           18.24\n",
      "delinq_2yrs                                   0.0\n",
      "earliest_cr_line                              Apr-2001\n",
      "inq_last_6mths                                1.0\n",
      "mths_since_last_delinq                        nan\n",
      "mths_since_last_record                        45.0\n",
      "open_acc                                      9.0\n",
      "pub_rec                                       1.0\n",
      "revol_bal                                     4341\n",
      "revol_util                                    10.3\n",
      "total_acc                                     34.0\n",
      "initial_list_status                           w\n",
      "out_prncp                                     2386.02\n",
      "out_prncp_inv                                 2386.02\n",
      "total_pymnt                                   167.02\n",
      "total_pymnt_inv                               167.02\n",
      "total_rec_prncp                               113.98\n",
      "total_rec_int                                 53.04\n",
      "total_rec_late_fee                            0.0\n",
      "recoveries                                    0.0\n",
      "collection_recovery_fee                       0.0\n",
      "last_pymnt_d                                  Feb-2019\n",
      "last_pymnt_amnt                               84.92\n",
      "next_pymnt_d                                  Mar-2019\n",
      "last_credit_pull_d                            Feb-2019\n",
      "collections_12_mths_ex_med                    0.0\n",
      "mths_since_last_major_derog                   nan\n",
      "policy_code                                   1\n",
      "application_type                              Individual\n",
      "annual_inc_joint                              nan\n",
      "dti_joint                                     nan\n",
      "verification_status_joint                     nan\n",
      "acc_now_delinq                                0.0\n",
      "tot_coll_amt                                  0.0\n",
      "tot_cur_bal                                   16901.0\n",
      "open_acc_6m                                   2.0\n",
      "open_act_il                                   2.0\n",
      "open_il_12m                                   1.0\n",
      "open_il_24m                                   2.0\n",
      "mths_since_rcnt_il                            2.0\n",
      "total_bal_il                                  12560.0\n",
      "il_util                                       69.0\n",
      "open_rv_12m                                   2.0\n",
      "open_rv_24m                                   7.0\n",
      "max_bal_bc                                    2137.0\n",
      "all_util                                      28.0\n",
      "total_rev_hi_lim                              42000.0\n",
      "inq_fi                                        1.0\n",
      "total_cu_tl                                   11.0\n",
      "inq_last_12m                                  2.0\n",
      "acc_open_past_24mths                          9.0\n",
      "avg_cur_bal                                   1878.0\n",
      "bc_open_to_buy                                34360.0\n",
      "bc_util                                       5.9\n",
      "chargeoff_within_12_mths                      0.0\n",
      "delinq_amnt                                   0.0\n",
      "mo_sin_old_il_acct                            140.0\n",
      "mo_sin_old_rev_tl_op                          212.0\n",
      "mo_sin_rcnt_rev_tl_op                         1.0\n",
      "mo_sin_rcnt_tl                                1.0\n",
      "mort_acc                                      0.0\n",
      "mths_since_recent_bc                          1.0\n",
      "mths_since_recent_bc_dlq                      nan\n",
      "mths_since_recent_inq                         2.0\n",
      "mths_since_recent_revol_delinq                nan\n",
      "num_accts_ever_120_pd                         0.0\n",
      "num_actv_bc_tl                                2.0\n",
      "num_actv_rev_tl                               5.0\n",
      "num_bc_sats                                   3.0\n",
      "num_bc_tl                                     3.0\n",
      "num_il_tl                                     16.0\n",
      "num_op_rev_tl                                 7.0\n",
      "num_rev_accts                                 18.0\n",
      "num_rev_tl_bal_gt_0                           5.0\n",
      "num_sats                                      9.0\n",
      "num_tl_120dpd_2m                              0.0\n",
      "num_tl_30dpd                                  0.0\n",
      "num_tl_90g_dpd_24m                            0.0\n",
      "num_tl_op_past_12m                            3.0\n",
      "pct_tl_nvr_dlq                                100.0\n",
      "percent_bc_gt_75                              0.0\n",
      "pub_rec_bankruptcies                          1.0\n",
      "tax_liens                                     0.0\n",
      "tot_hi_cred_lim                               60124.0\n",
      "total_bal_ex_mort                             16901.0\n",
      "total_bc_limit                                36500.0\n",
      "total_il_high_credit_limit                    18124.0\n",
      "revol_bal_joint                               nan\n",
      "sec_app_earliest_cr_line                      nan\n",
      "sec_app_inq_last_6mths                        nan\n",
      "sec_app_mort_acc                              nan\n",
      "sec_app_open_acc                              nan\n",
      "sec_app_revol_util                            nan\n",
      "sec_app_open_act_il                           nan\n",
      "sec_app_num_rev_accts                         nan\n",
      "sec_app_chargeoff_within_12_mths              nan\n",
      "sec_app_collections_12_mths_ex_med            nan\n",
      "sec_app_mths_since_last_major_derog           nan\n",
      "hardship_flag                                 N\n",
      "hardship_type                                 nan\n",
      "hardship_reason                               nan\n",
      "hardship_status                               nan\n",
      "deferral_term                                 nan\n",
      "hardship_amount                               nan\n",
      "hardship_start_date                           nan\n",
      "hardship_end_date                             nan\n",
      "payment_plan_start_date                       nan\n",
      "hardship_length                               nan\n",
      "hardship_dpd                                  nan\n",
      "hardship_loan_status                          nan\n",
      "orig_projected_additional_accrued_interest    nan\n",
      "hardship_payoff_balance_amount                nan\n",
      "hardship_last_payment_amount                  nan\n",
      "disbursement_method                           Cash\n",
      "debt_settlement_flag                          N\n",
      "debt_settlement_flag_date                     nan\n",
      "settlement_status                             nan\n",
      "settlement_date                               nan\n",
      "settlement_amount                             nan\n",
      "settlement_percentage                         nan\n",
      "settlement_term                               nan\n"
     ]
    }
   ],
   "source": [
    "# Example loan\n",
    "print(\"Example loan:\\n\")\n",
    "for col in df_loans.columns:\n",
    "    print(f\"{col:<45} {df_loans.iloc[0][col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan statuses with counts:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loan_status\n",
       "Fully Paid                                             1041952\n",
       "Current                                                 919695\n",
       "Charged Off                                             261655\n",
       "Late (31-120 days)                                       21897\n",
       "In Grace Period                                           8952\n",
       "Late (16-30 days)                                         3737\n",
       "Does not meet the credit policy. Status:Fully Paid        1988\n",
       "Does not meet the credit policy. Status:Charged Off        761\n",
       "Default                                                     31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loan statuses\n",
    "print(\"Loan statuses with counts:\\n\")\n",
    "df_loans['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "\n",
    "We define the loan statuses \"Charged Off\" and \"Default\" as **default**, and \"Fully Paid\" as **non-default**. Skip others for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loans after filtering loan statuses: 1303638\n",
      "Features: 145\n"
     ]
    }
   ],
   "source": [
    "# Filter statuses\n",
    "target_statuses = ['Fully Paid', 'Charged Off', 'Default']\n",
    "df_loans_filtered = df_loans[df_loans['loan_status'].isin(target_statuses)].copy()\n",
    "print(\"Number of loans after filtering loan statuses:\", df_loans_filtered.shape[0])\n",
    "print(\"Features:\", df_loans_filtered.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after target encoding:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1303638, 146)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode target: default = 1, non-default = 0\n",
    "df_loans_filtered['default'] = df_loans_filtered['loan_status'].isin(['Charged Off', 'Default']).astype(int)\n",
    "print(\"Shape after target encoding:\")\n",
    "df_loans_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-defaults: 1041952 (79.9%) \n",
      "Defaults: 261686 (20.1%) \n"
     ]
    }
   ],
   "source": [
    "n_loans = len(df_loans_filtered)\n",
    "non_default, default = df_loans_filtered['default'].value_counts().items()\n",
    "\n",
    "print(f\"Non-defaults: {non_default[1]} ({non_default[1] / n_loans:.1%}) \")\n",
    "print(f\"Defaults: {default[1]} ({default[1] / n_loans:.1%}) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features used for prediction\n",
    "\n",
    "Only variables that were certainly available at the time of loan origination were included. Since most variables were uncertain in this respect, only five features were used: (descriptions from LCDataDictionary.xlsx)\n",
    "\n",
    "- **annual_inc**         - The self-reported annual income provided by the borrower during registration\n",
    "\n",
    "- **application_type**   - Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
    "\n",
    "- **dti**                - A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income\n",
    "\n",
    "- **home_ownership**     - The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
    "\n",
    "- **purpose**            - A category provided by the borrower for the loan request\n",
    "\n",
    "Example of variables with uncertain time measurements:\n",
    "\n",
    "- **delinq_2yrs**   - The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.  \n",
    "\n",
    "- **loan_amnt**    - The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value  \n",
    "\n",
    "Other:\n",
    "\n",
    "- **loan_status** - Current status of the loan      \n",
    "    *(Target)*\n",
    "- **issue_d** - The month which the loan was funded     \n",
    "    *(Used for time-based splitting)*    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303638, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_vars = [\n",
    "    'annual_inc',\n",
    "    'application_type',\n",
    "    'dti',\n",
    "    'home_ownership',\n",
    "    'purpose',\n",
    "    'default',  # target\n",
    "    'issue_d',  # for time-based splitting\n",
    "]\n",
    "df_model = df_loans_filtered[selected_vars].copy()\n",
    "df_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables with missing values:\n",
      "\n",
      "dti: 312 (2.39%)\n"
     ]
    }
   ],
   "source": [
    "missing_counts = df_model.isna().sum().sort_values(ascending=False)\n",
    "missing_vars = missing_counts[missing_counts > 0]\n",
    "\n",
    "total_rows = len(df_model)\n",
    "\n",
    "if len(missing_vars) > 0:\n",
    "    print(\"Variables with missing values:\\n\")\n",
    "    for col, count in missing_vars.items():\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col}: {count} ({percentage:.2%})\")\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing dti-values with median of the other dtis\n",
    "numeric_cols = df_model.select_dtypes(include=['number']).columns\n",
    "df_model[numeric_cols] = df_model[numeric_cols].fillna(df_model[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found\n"
     ]
    }
   ],
   "source": [
    "# Check missing values again\n",
    "missing_counts = df_model.isna().sum().sort_values(ascending=False)\n",
    "missing_vars = missing_counts[missing_counts > 0]\n",
    "\n",
    "total_rows = len(df_model)\n",
    "\n",
    "if len(missing_vars) > 0:\n",
    "    print(\"Variables with missing values:\\n\")\n",
    "    for col, count in missing_vars.items():\n",
    "        percentage = (count / total_rows) * 100\n",
    "        print(f\"{col}: {count} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features with counts:\n",
      "\n",
      "purpose\n",
      "debt_consolidation    757610\n",
      "credit_card           285708\n",
      "home_improvement       84497\n",
      "other                  74937\n",
      "major_purchase         28328\n",
      "medical                15024\n",
      "small_business         15010\n",
      "car                    14121\n",
      "moving                  9173\n",
      "vacation                8732\n",
      "house                   6967\n",
      "wedding                 2294\n",
      "renewable_energy         911\n",
      "educational              326\n",
      "Name: count, dtype: int64\n",
      "\n",
      "home_ownership\n",
      "MORTGAGE    645509\n",
      "RENT        517821\n",
      "OWN         139849\n",
      "ANY            267\n",
      "OTHER          144\n",
      "NONE            48\n",
      "Name: count, dtype: int64,\n",
      "\n",
      "application_type\n",
      "Individual    1280394\n",
      "Joint App       23244\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique categories: 22\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "print(\"Categorical features with counts:\\n\")\n",
    "\n",
    "print(f\"{df_loans_filtered['purpose'].value_counts()}\\n\")\n",
    "print(f\"{df_loans_filtered['home_ownership'].value_counts()},\\n\")\n",
    "print(f\"{df_loans_filtered['application_type'].value_counts()}\\n\")\n",
    "\n",
    "# Total unique categories across all three categorical features\n",
    "total_categories = (\n",
    "    df_loans_filtered['purpose'].nunique() +\n",
    "    df_loans_filtered['home_ownership'].nunique() +\n",
    "    df_loans_filtered['application_type'].nunique()\n",
    ")\n",
    "print(\"Total unique categories:\", total_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loans after one-hot encoding: 1303638\n",
      "Number of features after one-hot encoding: 23\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['purpose', 'home_ownership', 'application_type']\n",
    "df_model = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Number of loans after one-hot encoding: {df_model.shape[0]}\")\n",
    "print(f\"Number of features after one-hot encoding: {df_model.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based Train/test split\n",
    "df_sorted = df_model.sort_values('issue_d')\n",
    "\n",
    "split_point = int(0.8 * len(df_sorted))\n",
    "\n",
    "\n",
    "\n",
    "X_train = df_sorted.iloc[:split_point].drop(['default', 'issue_d'], axis=1)\n",
    "y_train = df_sorted.iloc[:split_point]['default']\n",
    "X_test = df_sorted.iloc[split_point:].drop(['default', 'issue_d'], axis=1)\n",
    "y_test = df_sorted.iloc[split_point:]['default']\n",
    "\n",
    "print(f\"Train period: {df_sorted['issue_d'].iloc[0]} to {df_sorted['issue_d'].iloc[split_point-1]}\")\n",
    "print(f\"Test period: {df_sorted['issue_d'].iloc[split_point]} to {df_sorted['issue_d'].iloc[-1]}\")\n",
    "\n",
    "print(f\"\\nNumber of loans in train period: {len(X_train)}\")\n",
    "print(f\"Number of loans in test period: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize features (mean = 0, sd = 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on train set\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "The logistic regression model estimates the probability of default for each observation as\n",
    "\n",
    "$$\n",
    "p(x_1, \\dots, x_m) = \\frac{1}{1 + \\exp\\big(-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_m x_m)\\big)},\n",
    "$$\n",
    "\n",
    "where $x_1, \\dots, x_m$ are the input features, and $\\beta_0, \\dots, \\beta_m$ are the coefficients to be estimated using the training data.\n",
    "\n",
    "The formula above can be rewritten as\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p(\\bf{x})}{1 - p(\\bf{x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_m x_m,\n",
    "$$\n",
    "\n",
    "where $\\textbf{x} = (x_1, \\dots, x_m)$. This formula shows that inputs $x_i$ that correspond to positive coefficients $\\beta_i$ are positively correlated with the probability $p(\\bf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "class_thr = 0.5\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] # probabilities\n",
    "y_pred = (y_pred_proba >= class_thr).astype(int) # binary predictions\n",
    "\n",
    "print(\"Training and predictions done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "p = np.mean(y_test) # fraction classified as 1 (default)\n",
    "n_0 = len(y_pred) - np.sum(y_pred)\n",
    "n_0_frac = n_0 / len(y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Majority-class-acc: {max(p, 1-p):.3f}\")\n",
    "print(f\"Number of predictions equal to 0: {n_0} ({n_0_frac:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "The above metrics are calcuated using a classification threshold of 0.5. Due to the imbalanced class proportions, most loans are therefore predicted as non-defaults. This causes the recall for defaults to be 0. If it is more important to correctly classify default than having good accuracy, a lower threshold should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUC, Gini\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "gini = 2 * auc - 1\n",
    "print(f\"ROC AUC: {auc:.3f}\")\n",
    "print(f\"Gini: {gini:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "ks_index = (tpr - fpr).argmax() # KS\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(fpr[ks_index], tpr[ks_index], color='red', s=30, label='KS point', zorder=5)\n",
    "\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--') # random classifier line\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** ROC curve showing the trade-off between TPR and FPR, with the KS point highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS point\n",
    "ks = max(tpr - fpr)\n",
    "print(f\"KS: {ks:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability/Monitoring Metrics\n",
    "\n",
    "#### PSI (Population Stability Index)\n",
    "\n",
    "(todo: fix this text)\n",
    "The Population Stability Index (PSI), which measures population drift, is 0.002 (10 bins; comparing predicted probabilities for training and test set). This indicates that the distributions of predicted probabilities for the training and test sets are very similar, which is confirmed by the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate PSI\n",
    "import numpy as np\n",
    "\n",
    "n_bins = 10\n",
    "\n",
    "# Predicted probabilities for the training set\n",
    "y_pred_proba_train = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "\n",
    "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "df_PSI_test = pd.DataFrame({'y_pred_proba': y_pred_proba})\n",
    "df_PSI_test['bin'] = pd.cut(df_PSI_test['y_pred_proba'], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "df_test_bins = (\n",
    "    df_PSI_test.groupby('bin', observed=False)\n",
    "    .agg(count=('y_pred_proba', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_test_bins['fraction_test'] = df_test_bins['count'] / df_test_bins['count'].sum()\n",
    "\n",
    "df_PSI_train = pd.DataFrame({'y_pred_proba_train': y_pred_proba_train})\n",
    "\n",
    "df_PSI_train['bin'] = pd.cut(df_PSI_train['y_pred_proba_train'], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "df_train_bins = (\n",
    "    df_PSI_train.groupby('bin', observed=False)\n",
    "    .agg(count=('y_pred_proba_train', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_train_bins['fraction_train'] = df_train_bins['count'] / df_train_bins['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_psi(train_frac, test_frac):\n",
    "    \"\"\"\n",
    "    train_frac: array of fractions per bin in the baseline population\n",
    "    test_frac: array of fractions per bin in the new population\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # to avoid division by zero\n",
    "    psi_values = (train_frac - test_frac) * np.log((train_frac + epsilon) / (test_frac + epsilon))\n",
    "    return np.sum(psi_values)\n",
    "\n",
    "psi = calculate_psi(df_train_bins['fraction_train'], df_test_bins['fraction_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSI plot code\n",
    "plt.figure(figsize=(5, 3))\n",
    "x_mid = df_test_bins['bin'].apply(lambda x: x.mid).to_numpy()\n",
    "plt.plot(x_mid, df_train_bins['fraction_train'].to_numpy(), label='Train', marker='o')\n",
    "plt.plot(x_mid, df_test_bins['fraction_test'].to_numpy(), label='Test', marker='o')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(np.arange(0, 1.01, 0.1))  # ticks at 0.0, 0.1, 0.2, ..., 1.0\n",
    "plt.xlabel(\"Probability bin\")\n",
    "plt.ylabel(\"Fraction of samples\")\n",
    "plt.title(\"Distribution of predicted probabilities (Train vs Test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3:** Plot confirming the small PSI value (the training curve is barely visible). The plot also shows that most predicted probabilities lie between 0 and 0.5, reflecting the class imbalance (80% non-defaults)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calibration\n",
    "df_calibration = pd.DataFrame({\n",
    "    'y_test': y_test,\n",
    "    'y_pred_proba': y_pred_proba\n",
    "})\n",
    "\n",
    "n_bins_cali = 10\n",
    "bin_edges = np.linspace(0, 1, n_bins_cali + 1)\n",
    "\n",
    "df_calibration[\"bin\"] = pd.cut(df_calibration['y_pred_proba'], bins=bin_edges, include_lowest=True)\n",
    "calibration_table = df_calibration.groupby('bin', observed=False).agg(\n",
    "    avg_pred=('y_pred_proba', 'mean'),          # Average predicted PD in the bin\n",
    "    obs_default_rate=('y_test', 'mean'),        # Fraction of defaults in the bin\n",
    ").reset_index() # make the bins a column instead of index, and use standard index\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot([0,1], [0,1], 'k--', label='Perfect calibration')\n",
    "plt.plot(calibration_table['avg_pred'].to_numpy(), calibration_table['obs_default_rate'].to_numpy(), marker='o', label='Model')\n",
    "plt.xlabel('Average Predicted PD')\n",
    "plt.ylabel('Observed Default Rate')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 4:** The calibration plot shows how well predicted probabilities correspond to the actual fraction of defaulters. For example, if the average predicted probability in the bin [0, 0.1] is 0.06, the fraction of actual defaulters among loans with predictions in [0, 0.1] should also be approximately 0.06.\n",
    "\n",
    "As we can see, the calibration starts failing at the same spot as where the distribution of predicted probabilities (plot above) approaches zero. One reason for this is that since there are few samples at higher probabilities, the fractions get more sensitive to non-default outliers with high probability predictions, causing the model to overestimate probabilities in the upper range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "top_n_features = 5\n",
    "feature_importance.head(top_n_features)\n",
    "\n",
    "print(f\"Top {top_n_features} features:\\n\")\n",
    "for i in range(top_n_features):\n",
    "    print(f\"{feature_importance.iloc[i]['Feature']:<30}{feature_importance.iloc[i]['Coefficient']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
